<%-
  # get _list_partitions
  p_cmd = "/opt/pbs/default/bin/qstat -Q | grep Rou | grep -v -E 'gpu|c1|c2|osg' | awk '{print $1}' | sort"
  begin
    output, status = Open3.capture2e(p_cmd)
    if status.success?
      _list_partitions = output.split("\n").map(&:strip).reject(&:blank?).sort
    else
      raise output
    end
  rescue => e
    _list_partitions = []
    p_error = e.message.strip
  end

-%>
---

# **MUST** set cluster id here that matches cluster configuration file located
# under /etc/ood/config/clusters.d/*.yml

cluster: "palmetto"

attributes:
  modules: "anaconda3/2019.10-gcc/8.3.1"

  extra_jupyter_args: ""
  
  # Palmetto PBS specific options
  pbs_select:
    label: "Number of resource chunks (select)"
    value: "1"
      
  pbs_ncpus:
    label: "CPU cores per chunk (ncpus)"
    value: "1"
    help: |
      <small> - Typical Palmetto compute nodes have <b>8, 12, 16, 20, 24, 28, 40, and 56</b> cores.<br>
      - DGX nodes have <b>128</b> cores.<br>
      - Bigmem nodes have <b>24, 32, 40, and 80</b> cores.
      - <b>Users can request any number of cores that is smaller than the number of available cores.</b>
      </small>
      
  pbs_mem:
    label: "Amount of memory per chunk (mem)"
    value: "1gb"
    help: |
      <small> - Typical Palmetto compute nodes have <b>15, 30, 46, 62, 125, 372, 748, and 990</b> gb of memory.<br>
      - DGX nodes have <b>990</b> gb of memory.<br>
      - Bigmem nodes have <b>500 and 750</b> gb and <b>1 and 1.5</b> tb of memory.
      </small>

  pbs_ngpus:
    label: "Number of GPUs per chunk (ngpus)"
    widget: "select"
    value: "0"
    options:
      - ["None",""]
      - ["1",":ngpus=1"]
      - ["2",":ngpus=2"]
      - ["4",":ngpus=4"]

  pbs_gpu_model:
    label: "GPU Model (gpu_model)"
    widget: "select"
    value: "none"
    options:
      - ["None",""]
      - ["Any", ":gpu_model=any"]
      - ["K20",":gpu_model=k20"]
      - ["K40",":gpu_model=k40"]
      - ["P100",":gpu_model=p100"]
      - ["V100",":gpu_model=v100"]
      - ["V100 with NVLink",":gpu_model=v100nv"]

  pbs_interconnect:
    label: "Interconnect"
    widget: "select"
    value: "any"
    options:
      - ["any",""]
      - ["1g - Ethernet older phases 1-6",":interconnect=1g"]
      - ["10g - Ethernet phase 7-18 and phase 0 (bigmem queue)","interconnect=10ge"]
      - ["25g - Ethernet phase 18b-27",":interconnect=25ge"]
      - ["56g - Ethernet phase 7-17",":interconnect=56g"]
      - ["fdr - Infiniband phase 7-17",":interconnect=fdr"]
      - ["hdr - Infiniband phase 18 and above",":interconnect=hdr"]
      - ["100g - Ethernet phase 18 and above",":interconnect=100g"]

  pbs_walltime:
    label: "Walltime"
    value: "00:30:00"
    help: |
      <small> - Walltime format is <b>hh:mm:ss</b>.<br>
      - Phase 1 through 6 nodes can be reserved up to 336 hours.<br> 
      - Phase 7 through 27 nodes can be reserved up to 72 hours. 
      </small>
      
  pbs_extras:
    label: "Extra PBS resource allocation request"
    value: ""
    help: |
      <small>- Enter the additional resource request just like how you would in a command line environment.<br>
      - Each request should start with a colon <b>:</b> sign. <br>
      - For example: <b>:chip_type=e5-2665</b></small>
      
  pbs_queue:
    label: "Queue"
    required: true
    value: "work1"
    help: |
      <small>Queue to submit the job to</small>
    <%- if p_error -%>
      <span class="text-danger">Error when parsing queues:</span>
      ```
      <%= p_error.gsub("\n", "\n      ") %>
      ```
    <%- end -%>
    <%- if _list_partitions.blank? -%>
    widget: text_field
    <%- else -%>
    widget: select
    options:
      <%- _list_partitions.each do |q| -%>
      - [ "<%= q %>", "<%= q %>" ]
      <%- end -%>
    <%- end -%>
    
form:
  - modules
  - extra_jupyter_args
  - pbs_select
  - pbs_ncpus
  - pbs_mem
  - pbs_ngpus
  - pbs_gpu_model
  - pbs_interconnect
  - pbs_extras
  - pbs_walltime
  - pbs_queue
  - bc_email_on_started
